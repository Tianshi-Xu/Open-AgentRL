import pandas as pd
import numpy as np
from transformers import AutoTokenizer
from tqdm import tqdm
import torch
import sys

def series_to_item(ls):
    """ä»pandas Series/numpy arrayä¸­æå–å®é™…å€¼ï¼ˆå®Œå…¨å‚è€ƒverlçš„å®ç°ï¼‰"""
    import numpy
    import pandas
    
    while isinstance(ls, (pandas.core.series.Series, numpy.ndarray)) and len(ls) == 1:
        ls = ls[0]
    return ls

def convert_nested_value_to_list_recursive(data_item):
    """é€’å½’è½¬æ¢åµŒå¥—å€¼ä¸ºlistï¼ˆå‚è€ƒverlçš„å®ç°ï¼‰"""
    if isinstance(data_item, dict):
        return {k: convert_nested_value_to_list_recursive(v) for k, v in data_item.items()}
    elif isinstance(data_item, list):
        return [convert_nested_value_to_list_recursive(elem) for elem in data_item]
    elif isinstance(data_item, np.ndarray):
        return convert_nested_value_to_list_recursive(data_item.tolist())
    else:
        return data_item

# åŠ è½½tokenizerï¼ˆä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„æ¨¡å‹è·¯å¾„ï¼‰
MODEL_PATH = "/home/v-tianshixu/pretrained_model/Qwen3-4B-Instruct-2507"
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    print(f"âœ… æˆåŠŸåŠ è½½tokenizer")
except Exception as e:
    print(f"âŒ åŠ è½½tokenizerå¤±è´¥: {e}")
    sys.exit(1)

# åŠ è½½æ•°æ®ï¼ˆå®Œå…¨æŒ‰ç…§verlçš„æ–¹å¼ï¼‰
try:
    df = pd.read_parquet("dataset/Open-AgentRL-SFT-3K/full_sft_3k_shuffled_v4.parquet")
    print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ï¼Œæ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"åˆ—å: {list(df.columns)}")
except Exception as e:
    print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
    sys.exit(1)

# æ£€æŸ¥å¿…éœ€çš„åˆ—
messages_key = "messages"
if messages_key not in df.columns:
    print(f"âŒ æ•°æ®ä¸­ç¼ºå°‘'{messages_key}'åˆ—")
    print(f"å¯ç”¨çš„åˆ—: {list(df.columns)}")
    sys.exit(1)

# æŒ‰ç…§verlçš„æ–¹å¼æå–messagesï¼ˆå…³é”®ï¼ï¼‰
print("æ­£åœ¨æå–messagesï¼ˆæŒ‰ç…§verlçš„æ–¹å¼ï¼‰...")
messages_list = df[messages_key].apply(series_to_item).tolist()
# å¤„ç†numpy arrayçš„æƒ…å†µï¼šè½¬æ¢ä¸ºlist
for i, msg in enumerate(messages_list):
    if isinstance(msg, np.ndarray):
        messages_list[i] = msg.tolist()
print(f"âœ… æˆåŠŸæå– {len(messages_list)} ä¸ªæ ·æœ¬çš„messages")

# æå–toolsï¼ˆå¦‚æœå­˜åœ¨ï¼‰
tools_list = None
tools_key = "tools"
if tools_key in df.columns:
    tools_list = df[tools_key].apply(convert_nested_value_to_list_recursive).tolist()
    print(f"âœ… æˆåŠŸæå–tools")
else:
    print(f"âš ï¸  æ•°æ®ä¸­æ²¡æœ‰'{tools_key}'åˆ—ï¼Œå°†ä½¿ç”¨None")

print("\næ­£åœ¨tokenize...")

# çœŸå®tokenizeæ¯ä¸ªæ ·æœ¬
lengths = []
errors = []

for idx in tqdm(range(len(messages_list)), desc="å¤„ç†æ ·æœ¬"):
    try:
        messages = messages_list[idx]
        
        # æœ€ç»ˆç¡®ä¿messagesæ˜¯listï¼ˆå¤„ç†numpy arrayç­‰ï¼‰
        if isinstance(messages, np.ndarray):
            messages = messages.tolist()
        if not isinstance(messages, list):
            raise ValueError(f"Messages should be a list, got {type(messages)}, value: {messages}")
        
        # éªŒè¯messagesæ ¼å¼
        if len(messages) == 0:
            raise ValueError("Messages list is empty")
        if not all(isinstance(msg, dict) and "role" in msg and "content" in msg for msg in messages):
            raise ValueError(f"Invalid message format: {messages[0] if messages else None}")
        
        # è·å–toolsï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        tools = None
        if tools_list is not None:
            tools = tools_list[idx]
        
        # ä½¿ç”¨apply_chat_templateçœŸå®tokenizeï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
        result = tokenizer.apply_chat_template(
            messages,
            tools=tools,
            tokenize=True,
            return_tensors="pt",
            add_generation_prompt=False,
        )
        
        # å¤„ç†è¿”å›å€¼ï¼ˆå¯èƒ½æ˜¯tensoræˆ–listï¼‰
        if isinstance(result, torch.Tensor):
            tokens = result
        elif isinstance(result, list):
            tokens = torch.tensor(result)
            if tokens.dim() == 1:
                tokens = tokens.unsqueeze(0)
        else:
            raise ValueError(f"Unexpected return type from apply_chat_template: {type(result)}")
        
        # è·å–çœŸå®é•¿åº¦
        if tokens.dim() == 1:
            seq_length = tokens.shape[0]
        elif tokens.dim() == 2:
            seq_length = tokens.shape[1]
        else:
            raise ValueError(f"Unexpected token tensor shape: {tokens.shape}")
        
        lengths.append(seq_length)
        
    except Exception as e:
        errors.append((idx, str(e)))
        if len(errors) <= 5:  # åªæ‰“å°å‰5ä¸ªé”™è¯¯
            print(f"\nâŒ é”™è¯¯æ ·æœ¬ {idx}: {e}")
            import traceback
            traceback.print_exc()

# ç»Ÿè®¡ç»“æœ
if lengths:
    lengths = torch.tensor(lengths, dtype=torch.long)
    
    print("\n" + "="*50)
    print("åºåˆ—é•¿åº¦ç»Ÿè®¡ï¼ˆçœŸå®tokenæ•°ï¼‰:")
    print("="*50)
    print(f"æ ·æœ¬æ€»æ•°: {len(lengths)}")
    print(f"å¹³å‡é•¿åº¦: {lengths.float().mean().item():.0f} tokens")
    print(f"ä¸­ä½æ•°: {lengths.median().item():.0f} tokens")
    print(f"æœ€å°å€¼: {lengths.min().item():.0f} tokens")
    print(f"æœ€å¤§å€¼: {lengths.max().item():.0f} tokens")
    print(f"æ ‡å‡†å·®: {lengths.float().std().item():.0f} tokens")
    
    # ç»Ÿè®¡è¶…è¿‡ç‰¹å®šé•¿åº¦çš„æ ·æœ¬æ•°é‡
    print("\nè¶…é•¿æ ·æœ¬ç»Ÿè®¡:")
    count_16k = (lengths > 16384).sum().item()
    pct_16k = count_16k / len(lengths) * 100
    print(f"  >16384 tokens: {count_16k} æ ·æœ¬ ({pct_16k:.2f}%)")
    
    count_32k = (lengths > 32768).sum().item()
    pct_32k = count_32k / len(lengths) * 100
    print(f"  >32768 tokens: {count_32k} æ ·æœ¬ ({pct_32k:.2f}%)")
    
    if count_32k > 0:
        max_length_val = lengths.max().item()
        print(f"  æœ€é•¿æ ·æœ¬: {max_length_val:.0f} tokens")
    
    print("\nåˆ†ä½æ•°:")
    for p in [50, 75, 90, 95, 99]:
        val = torch.quantile(lengths.float(), p/100).item()
        print(f"  {p}%åˆ†ä½: {val:.0f} tokens")
    
    print("\né•¿åº¦åˆ†å¸ƒ:")
    # ä½¿ç”¨æ‰‹åŠ¨è®¡ç®—æ¥åŒ…å«>32Kçš„æ ·æœ¬
    bins_list = [0, 1024, 2048, 4096, 8192, 16384, 32768, float('inf')]
    bin_labels = ["<1K", "1K-2K", "2K-4K", "4K-8K", "8K-16K", "16K-32K", ">32K"]
    
    # æ‰‹åŠ¨è®¡ç®—æ¯ä¸ªåŒºé—´çš„æ•°é‡
    hist_counts = []
    for i in range(len(bins_list) - 1):
        left = bins_list[i]
        right = bins_list[i + 1]
        if right == float('inf'):
            count = ((lengths >= left).sum()).item()
        else:
            count = ((lengths >= left) & (lengths < right)).sum().item()
        hist_counts.append(count)
    
    for label, count in zip(bin_labels, hist_counts):
        pct = count / len(lengths) * 100
        print(f"  {label:>8}: {count:>5} æ ·æœ¬ ({pct:>5.1f}%)")
    
    # å»ºè®®çš„max_length
    p95 = torch.quantile(lengths.float(), 0.95).item()
    p99 = torch.quantile(lengths.float(), 0.99).item()
    print("\nå»ºè®®çš„max_lengthè®¾ç½®:")
    print(f"  è¦†ç›–95%æ ·æœ¬: {p95:.0f} tokens")
    print(f"  è¦†ç›–99%æ ·æœ¬: {p99:.0f} tokens")
    print(f"  å½“å‰è®¾ç½®: 32768 tokens")
    if p95 > 32768:
        print(f"  âš ï¸  è­¦å‘Š: 95%åˆ†ä½è¶…è¿‡å½“å‰max_lengthï¼Œå¯èƒ½ä¸¢å¤±æ•°æ®ï¼")
    elif p95 > 16384:
        print(f"  ğŸ’¡ å»ºè®®: å¯è€ƒè™‘ä½¿ç”¨ {int(p95*1.1):.0f} tokens")
    else:
        print(f"  âœ… å½“å‰è®¾ç½®å……è¶³")
        
    # æ˜¾å­˜ä¼°ç®—ï¼ˆæ›´å‡†ç¡®çš„ä¼°ç®—ï¼‰
    print("\næ˜¾å­˜éœ€æ±‚ä¼°ç®—ï¼ˆå•GPU, 4Bæ¨¡å‹, batch_size=1ï¼‰:")
    avg_length = lengths.float().mean().item()
    print(f"  åºåˆ—é•¿åº¦: {avg_length:.0f} tokens (å¹³å‡)")
    print(f"  æ¨¡å‹å‚æ•°: 8 GB (bf16)")
    print(f"  æ¢¯åº¦: 8 GB (bf16)")
    print(f"  ä¼˜åŒ–å™¨: 32 GB (AdamW fp32)")
    # æ¿€æ´»å€¼ä¼°ç®—ï¼šhidden_size=2560, num_layers=36, checkpointingåçº¦ä¿å­˜5%çš„æ¿€æ´»
    # æ¯ä¸ªcheckpoint: batch * seq_len * hidden_size * 2 bytes (bf16)
    # ä¼°ç®—åŒæ—¶æœ‰å¤šä¸ªcheckpointåœ¨å†…å­˜
    est_activation_per_checkpoint = 1 * avg_length * 2560 * 2 / (1024**3)  # GB per checkpoint
    est_active_checkpoints = 2  # ä¼°ç®—åŒæ—¶æœ‰2-3ä¸ªcheckpoint
    est_activation = est_activation_per_checkpoint * est_active_checkpoints * 0.05  # checkpointingèŠ‚çœ
    est_activation = max(est_activation, 1.0)  # è‡³å°‘1GB
    print(f"  æ¿€æ´»å€¼: ~{est_activation:.1f} GB (gradient checkpointingåä¼°ç®—)")
    print(f"  æ€»è®¡: ~{48 + est_activation:.1f} GB")

if errors:
    print(f"\nâš ï¸  å¤„ç†å¤±è´¥çš„æ ·æœ¬æ•°: {len(errors)}")
    if len(errors) > 5:
        print(f"   (ä»…æ˜¾ç¤ºå‰5ä¸ªé”™è¯¯)")

print("\n" + "="*50)